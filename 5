import pandas as pd

mydata=pd.read_csv('/content/diabetes.csv')
Independent_features=mydata.iloc[:,0:8]
Dependent_features=mydata.iloc[:,8]
# mydata.head()

# pd.DataFrame(mydata).shape

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(Independent_features,Dependent_features,test_size=0.20,random_state=1)

# from xgboost import XGBClassifier
# from sklearn.linear_model import LogisticRegression
# from sklearn.neighbors import KNeighborsClassifier
# from sklearn.svm import SVC

# models=[]
# models.append(('LogisticRegression',LogisticRegression(max_iter=1000)))
# models.append(('KNN',KNeighborsClassifier()))
# models.append(('SVM',SVC()))
# models.append(('XGB',XGBClassifier(eta=0.1,gamma=10)))

# import time
# from sklearn.metrics import accuracy_score
# results=[]
# names=[]
# scoring='accuracy'

# for name,model in models:
#     start_time=time.time()
#     model.fit(X_train,y_train)

#     y_pred=model.predict(X_test)
#     predictions=[round(value) for value in y_pred]
#     accuracy =accuracy_score(y_test,predictions)
#     print("Accuracy: %.2f%%" %(accuracy*100.0),name)
#     print("--- %s seconds ---" % (time.time()-start_time))

from xgboost import XGBClassifier
import time
from sklearn.metrics import accuracy_score

# Create an XGBoost model
model = XGBClassifier(eta=0.1, gamma=10)

start_time = time.time()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
predictions = [round(value) for value in y_pred]
accuracy = accuracy_score(y_test, predictions)
print("Accuracy: %.2f%%" % (accuracy * 100.0))
print("--- %s seconds ---" % (time.time() - start_time))
